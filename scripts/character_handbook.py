from langchain.chains import RetrievalQA
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import ast
import os 


# Step 1: Load character file
text_file = "scripts/outputs/jubensha/character.txt"

character=[]
with open(text_file,'r') as f: 
    character_file=f.read()
    character.append(character_file)


# Initialize the LLM model with the correct version
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.7)

# Prepare the prompt template
prompt_template = PromptTemplate(
    input_variables=["text"], 
    template="Given the following text, extract the names of all characters:\n{text}. For example: ['Character Name 1', 'Character Name 2', 'Character Name 3']  "
)

# Initialize the chain
llm_chain = LLMChain(llm=llm, prompt=prompt_template)

# Run the chain with the text input
response = llm_chain.run(character)

# Convert the string to a list
character_list = ast.literal_eval(response)

# Step 1: Load all generated text
text_files = [
    "scripts/outputs/jubensha/background_setting.txt",
    "scripts/outputs/jubensha/character.txt",
    "scripts/outputs/jubensha/character_event_log.txt",
    "scripts/outputs/jubensha/player_instructions.txt",
    "scripts/outputs/jubensha/player_clues.txt",
    "scripts/outputs/jubensha/title.txt",
    "scripts/outputs/jubensha/time_taken.txt",
    "scripts/outputs/host_instructions_handbook.txt"
]

documents = []
for file_path in text_files:
    with open(file_path, 'r') as f:
        content = f.read()
        documents.append(content)

# Step 2: Split and embed
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
docs = text_splitter.create_documents(documents)

embedder = HuggingFaceEmbeddings(model_name="BAAI/bge-small-en-v1.5")
db = FAISS.from_documents(docs, embedder)

retriever = db.as_retriever()

# Step 3: Setup the RetrievalQA Chain
qa_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.7),
    retriever=retriever,
    return_source_documents=False
)


sections = [
    "ç¬¬ä¸€å¹•",
    "ç¬¬äºŒå¹•",
    "ç¬¬ä¸‰å¹•",
    "æœ€ç»ˆå¹•"
]


# Use a for loop generate character handbooks for all characters 
for character in character_list:
    # Initialize the content for the handbook
    handbook = {}

    # Generate each section one by one
    for section in sections:
        prompt = f"""
        ä½ æ˜¯ä¸€åä¸“ä¸šçš„ã€Šå‰§æœ¬æ€ã€‹ç¼–å‰§ã€‚è¯·ä¸ºä¸€ä¸ªæ²‰æµ¸å¼å‰§æœ¬æ€æ¸¸æˆä¸­çš„è§’è‰²ç”Ÿæˆå®Œæ•´çš„è§’è‰²å‰§æœ¬ã€‚

        å‰§æœ¬åº”å…·æœ‰å®Œæ•´çš„äººç‰©èƒŒæ™¯ã€æƒ…æ„ŸåŠ¨æœºã€ä¸ä»–äººçš„å…³ç³»ï¼Œå¹¶ä»¥â€œå¹•â€ä½œä¸ºç»“æ„ï¼Œæ¨åŠ¨å‰§æƒ…å‘å±•å’Œç©å®¶äº’åŠ¨ã€‚

        ##è§’è‰²å‰§æœ¬æ ¼å¼##


        ğŸ§â€â™‚ï¸ è§’è‰²åç§°
        ğŸ­ æ€§æ ¼ç‰¹ç‚¹ï¼ˆä¾‹å¦‚ï¼šå†²åŠ¨ã€å†·é™ã€å¤šç–‘ã€åœ†æ»‘ç­‰ï¼‰
        ğŸ§³ èƒŒæ™¯æ•…äº‹ï¼ˆè§’è‰²çš„èº«ä»½ã€ç»å†ã€ä¸æ¡ˆä»¶æˆ–å…¶ä»–è§’è‰²çš„è¿‡å¾€çº è‘›ç­‰ï¼‰

        ç„¶åï¼Œå°†å‰§æœ¬åˆ†ä¸ºä»¥ä¸‹å‡ å¹•ï¼š

        ğŸ“– ç¬¬ä¸€å¹•ï¼šäººç‰©ç™»åœºä¸å…³ç³»å»ºç«‹
        - è§’è‰²å½“å‰çš„çŠ¶æ€ä¸å¿ƒæƒ…
        - ä»–çŸ¥é“çš„ä¿¡æ¯
        - ä¸å…¶ä»–è§’è‰²çš„å…³ç³»ä¸çœ‹æ³•
        - æ­¤é˜¶æ®µçš„ç›®æ ‡å’Œéšè—çš„ç§˜å¯†

        ğŸ•µï¸ ç¬¬äºŒå¹•ï¼šå‰§æƒ…æ¨è¿›ä¸çŸ›ç›¾æ¿€åŒ–
        - å¾—åˆ°çš„æ–°çº¿ç´¢æˆ–æƒ…æŠ¥
        - ä¸å…¶ä»–äººçš„å†²çªæˆ–åˆä½œ
        - è¡ŒåŠ¨è®¡åˆ’æˆ–åº”å¯¹æ–¹å¼

        ğŸ­ ç¬¬ä¸‰å¹•ï¼šçœŸç›¸æµ®ç°ä¸è§’è‰²è½¬æŠ˜
        - çœŸç›¸çš„æ­éœ²æˆ–éƒ¨åˆ†çº¿ç´¢çš„æ‹¼å‡‘
        - æƒ…ç»ªå˜åŒ–ï¼Œå…³é”®å¯¹æŠ—
        - ä»–é€‰æ‹©æ­éœ²æˆ–éšç’çš„ä¿¡æ¯

        ğŸ§© æœ€ç»ˆå¹•ï¼šæ¨ç†æŠ•ç¥¨ä¸å‘½è¿ç»“å±€
        - æœ€ç»ˆè¡¨æ€ä¸è¾©è§£

        è¯·æŒ‰ç…§ä»¥ä¸Šæ ¼å¼åˆ›ä½œ{character}çš„è§’è‰²å‰§æœ¬çš„{section}
        
        è¯·ç”¨ä¸­æ–‡æ’°å†™ï¼Œè¯­è¨€ä¸“ä¸šï¼Œæ ¼å¼æ¸…æ™°ï¼Œé€‚åˆä¸»æŒäººç›´æ¥ä½¿ç”¨ã€‚
        """
        
        # Running the QA chain with the section-specific prompt
        response = qa_chain.run(prompt)
        
        # Store the generated section
        handbook[section] = response
    
    # æ„å»ºç›®æ ‡æ–‡ä»¶å¤¹è·¯å¾„
    output_dir = f"scripts/outputs/characters"
    os.makedirs(output_dir, exist_ok=True)  # ç¡®ä¿ç›®å½•å­˜åœ¨

    # æ„å»ºå®Œæ•´æ–‡ä»¶è·¯å¾„
    file_path = os.path.join(output_dir, f"{character}_handbook.txt")

    with open(file_path, "w", encoding="utf-8") as f:
        for section, content in handbook.items():
            f.write(f"{section}:\n{content}\n\n")
    
    print(f"Handbook has been saved to 'outputs/characters/{character}_handbook.txt'")

    


        








